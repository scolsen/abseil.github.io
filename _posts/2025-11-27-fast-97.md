---
title: "Performance Tip of the Week #97: Virtuous ecosystem cycles"
layout: fast
sidenav: side-nav-fast.html
published: true
permalink: fast/97
type: markdown
order: "097"
---

Originally posted as Fast TotW #97 on August 21, 2025

*By [Chris Kennelly](mailto:ckennelly@google.com)*

Updated 2025-11-27

Quicklink: [abseil.io/fast/97](https://abseil.io/fast/97)


Software ecosystems aim to maximize qualities like efficiency, correctness, and
reliability while minimizing the costs of achieving these properties. Improving
a single service through customization can help build an optimization more
expediently, but it has an inherent limited scope to its upside and increases
[technical debt](/fast/52). A point solution fails to provide the full benefits
from applying features horizontally. In this episode, we discuss how lessons
learned from partnerships to improve individual applications can improve
efficiency for everyone.

## Case studies

We look at several case studies where we could take a feature that showed
benefits for a single team and rolled it out to provide benefits for all of
Google.

### SwissMap

Abseil's
[hash table implementation](https://abseil.io/blog/20180927-swisstables),
SwissMap, originated out of a partnership between our C++ core libraries team
and two Zurich-based engineers, Alkis Evlogimenos and Roman Perepelitsa, on our
search indexing team. They had set out to make an improved hash table that used
open addressing for [fewer data indirections](/fast/83), improved the design to
reduce memory usage, and added API features to avoid unnecessary copies. Jeff
Dean and Sanjay Ghemawat had suggested a SIMD-accelerated control block to speed
up comparison, allowing the table to efficiently run at a higher load factor.

While hash tables are a key tool in a programmer's toolbox, they tend to not
span API boundaries to nearly the same degree as other
[vocabulary types](https://www.open-std.org/jtc1/sc22/wg21/docs/papers/2020/p2125r0.pdf)
like `std::string` and `std::vector`. For the indexing team's purposes, most of
the benefits for their application could be obtained by replacing their hottest
hashtables and declaring victory.

But rather than stopping, the team opted to drive widespread adoption of the
superior hashtable. Over tens of thousands of changes later, nearly every hash
table in Google's codebase is a SwissMap. The early years of the rollout saved a
substantial amount of CPU and RAM. These savings were only unlocked because we
pursued a scaled migration.

The broad rollout produced ongoing ecosystem dividends:

*   "Just use SwissMap" was good advice, but for an engineer trying to
    [develop a new feature](https://queue.acm.org/detail.cfm?id=3733703), it is
    easier to use the existing types, even if SwissMap would be a better fit for
    performance. The widespread usage of SwissMap made it easier for new code to
    reach for it.
*   Drawing on lessons from prior hash table changes, [randomization](/fast/93)
    was introduced to prevent code from relying on the order of iteration. This
    made it easier to land subsequent optimizations under the hood, allowing us
    to iteratively improve the hash function by changing its algorithm.
*   Because of our implementation freedom, we were able to add telemetry
    features like [built-in profiling](/fast/26) that work across the fleet.
    This has allowed us to find and fix bad hash functions, identify
    optimizations for small tables, and proactively `reserve` containers to
    their final sizes.

### Size class-driven allocation

Modern memory allocators like TCMalloc use
"[size classes](https://github.com/google/tcmalloc/blob/master/docs/design.md#objects)"
to efficiently cache small objects. Rather than fit each allocation precisely,
the allocator determines which size class a request falls into and checks the
freelist for that size class. When an object is freed, the allocator determines
which size class the memory belongs to and puts the object on the appropriate
freelist for future reuse. This design simplifies bookkeeping and reduces the
cost of allocating and deallocating memory.

#### Sized deallocation

Using size classes requires a memory allocator to determine which freelist to
put freed objects onto. TCMalloc's original mapping approach required
[several pointer dereferences](/fast/83). In 2012, Google's compiler
optimization team identified that the object's size is frequently already known
outside of the allocator. Passing this size as an
[argument to the allocator](https://www.open-std.org/jtc1/sc22/wg21/docs/papers/2013/n3778.html)
avoids the expensive lookup.

Once sized deallocation was implemented, several teams were able to adopt the
feature to reduce allocation costs. This optimization can exacerbate
[existing undefined behavior](https://github.com/google/tcmalloc/blob/master/docs/mismatched-sized-delete.md#tcmalloc-is-buggy)
or introduce bugs in the rare case of tail-padded structures, requiring would-be
adopters to clean up their code and transitive dependencies. Because of these
lurking issues, the default remained "unsized" delete.

Motivated by
[fleetwide profiles showing the horizontal cost of TCMalloc](https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/44271.pdf),
work began to enable sized deallocation for the entire codebase.

*   [Runtime assertions](/fast/93) were added to TCMalloc to catch bugs in debug
    builds.
*   [Test failures](https://abseil.io/resources/swe-book/html/ch11.html#:~:text=The%20Beyonc%C3%A9%20Rule&text=The%20straightforward%20answer%20is%3A%20test,an%20automated%20test%20for%20it.)
    were investigated and numerous fixes submitted.
*   The feature was progressively rolled out to Address Sanitizer, debug, and
    optimized builds, preventing backsliding.

Infrastructure improvements, like the runtime assertions themselves, cost the
same to develop for one team as they do would for widespread deployment. For
other steps, like enabling sized deallocation in tests, operating centrally on
all tests was cheaper and easier to do than asking each team to set up a
parallel set of tests for themselves and their dependencies.

While the project delivered on its original goal of reducing TCMalloc costs in
the fleet, the central effort strengthened the ecosystem's reliability as a
whole by creating a sturdier foundation:

*   By enabling sized deallocation for all tests, teams already relying on the
    optimization benefited from the prevention of new bugs in their transitive
    dependencies. In the past, bugs would manifest as production crashes
    requiring teams to play whack-a-mole to set up extra testing. Moving the
    entire platform to sized delete foreclosed this entire class of bugs while
    avoiding the cost of duplicative and ad hoc testing setups.
*   The added information allows TCMalloc to
    [occasionally double-check](https://dl.acm.org/doi/10.1145/3639477.3640328)
    the purported size, allowing further memory corruption bugs to be uncovered.
    This was only possible because size arguments are routinely passed to
    deallocations.
*   The centralized burndown uncovered a relatively rare, but important, pattern
    of tail padded structures not working well with sized deallocation. This
    spurred C++20's
    [destroying delete](https://www.open-std.org/jtc1/sc22/wg21/docs/papers/2018/p0722r3.html)
    feature, which may not have been standardized and adopted universally
    without a central vantage point of its value.

Had we eschewed centralized adoption, individual teams would have faced similar
challenges, more bugs in production, and not benefitted from the flywheel of
follow-on improvements.

#### Size class improvements

Size classes simplify the logic required to allocate and deallocate memory, but
they introduce a new optimization problem: which sizes should we select as our
buckets? For many years, these partitions were chosen to minimize the amount of
memory lost by rounding up the request to its bucket ("internal fragmentation")
based on fleetwide profiles.

Internal fragmentation is just [one metric](/fast/70) we can look at when making
these choices. Merging two size classes would mean some smaller sizes grow,
increasing roundup overhead, while being offset by reducing the amount of memory
left in TCMalloc's caches at peak demand
("[external fragmentation](https://storage.googleapis.com/gweb-research2023-media/pubtools/6213.pdf#page=4)").
Having too few size classes would put more stress on TCMalloc's backend and
global locks.

One partnership with the Search retrieval team produced a series of
optimizations. The experiments showed improved performance with two sets of size
classes, "power-of-2 below 64" and "power-of-2 only." Rather than stop with just
that service, we opted to evaluate the impact for all workloads.

The former struck a tradeoff between maximizing CPU performance while modestly
increasing RAM usage. Fleetwide A/B experimentation allowed this set to be
evaluated and rolled out to improve aggregate CPU performance, a result
comparable to the initial load tests motivating the work.

Continued investigation of the "power-of-2 only" result pointed towards reducing
the amount of time an object might sit on one of TCMalloc's caches before the
next allocation that uses it. By choosing size classes that were likely to see
high allocation rates rather than solely minimizing internal fragmentation, we
were able to further improve fleetwide CPU performance.

We can contrast this with the hypothetical counterfactual where we made size
class [customizability](/fast/52) a full-fledged feature of TCMalloc and
individual teams tuned for their workloads.

*   For the teams with the time and inclination to customize their
    configuration, they might have seen even larger wins for themselves than
    what they obtained because of the global rollout, but over time, these
    settings might have become stale. The second round of global optimizations
    underscores the risk that individual teams might have stopped at a
    suboptimal point.
*   More configurations make it harder to maintain TCMalloc, and hamper
    [implementation freedom](/fast/52).
*   The tightly controlled configuration space made it possible to land
    improvements centrally, without having to navigate the existing
    customizations or risks from [breaking](https://hyrumslaw.com)
    [someone's use case](https://xkcd.com/1172/).

### Turbo troubles

Early work to use AVX instructions in general purpose workloads faced the
headwind of downclocking on the first AVX-capable platforms. In a math-intensive
workload, a modest frequency reduction would be a tolerable tradeoff for a
doubling in floating point throughput. In other workloads, the compiler's
occasional use of them in mostly scalar code produced performance regressions,
not wins, which deterred adoption.

Changing the compiler to constrain autovectorization broke the logjam. It was
possible to make this change because:

*   Most math-intensive code already used intrinsics to ensure high-quality
    vectorization, rather than relying on autovectorization. This was unaffected
    by the compiler change.
*   Scalar code could unlock and leverage new instruction sets like
    [BMI2](/fast/39) from subsequent microarchitectures.
*   In places where mostly scalar code could be vectorized somewhat, the
    compiler was constrained from introducing heavyweight vector instructions
    that introduced downclocking.

Being able to leverage the performance benefits of these new instructions
without downsides spurred broader adoption, creating a flywheel for further
horizontally and vertically-driven optimizations to be built on top of it.

## Lessons

### Don't assume the status quo is fixed

Teams working on a single application sometimes treat the underlying ecosystem
and platform as immutable, and solve problems within that constraint. The
platform itself is rarely immutable if there's sufficient value to be gained
from an ecosystem-wide change.

Even if it makes sense to work around a problem for reasons of expediency,
surfacing these pain points can raise awareness of frequently encountered pain
points. A frequent problem might look rare if everyone works around it rather
than seeing it get fixed in the platform once and for all.

### Focus on problems and outcomes, not exact solutions

[Prototypes](/fast/72) are an invaluable resource for assessing how to solve a
problem and determining what strategies actually work. It can help to step back
to think about requirements and [desired outcomes](/fast/70). We can say "yes,
this is an important problem worth solving," while simultaneously saying "but in
a different way."

When charting a path for scaled adoption of a prototype, we might want to use a
different implementation strategy to target a broader addressable market, take
advantage of existing use cases, or avoid long-term technical debt. Rarely is
"deploy exactly this prototype" the outcome we care about, so flexibility
towards solving the problem in different ways can let us grow our impact.

### Create and use leverage

As we take vertically-inspired solutions and deploy them horizontally, we are
often looking for leverage. If there is already a broadly used library or
feature that we can improve, we get a larger benefit than if we created an
entirely new thing that needed adoption. Rather than workaround a problem, we
should see if we can push changes further down the stack to fix issues more
generally and for everyone.

At other times, we might need to create a lever for ourselves. As seen with
SwissMap and AVX, driving adoption helped us to bootstrap. It created usage
where there was none previously, and then used that lever to motivate further
centralized optimizations.

### Understand what is seen and unseen

Opinionated libraries that might offer relatively few configuration options may
appear to be an impediment to optimization, but it is easy to miss the
opportunity costs of catering too much to specific use cases or flexibility. The
costs of the former tend to be [visible](/fast/74), while the latter is not.

A healthier, more sustainable core library can deliver other optimizations that
help our service of interest. For example, we can see the costs of SwissMap's
per-instance [randomization](/fast/93) when copying.

*   The direct savings are clear: Removing this randomization would help improve
    performance for copying-intensive users.
*   The indirect costs are less so: Loss of randomization would make it harder
    to add optimizations to common operations like lookup and insertion. This
    would hurt the broad ecosystem, likely including copy-intensive users as
    well.

## Closing words

Deep work with individual workloads can produce novel optimization insights.
Finding ways to scale these to the broader ecosystem can increase our impact,
both by removing bottlenecks for that workload and by addressing the problem
systematically.
